---
title: "Prediction Practice"
author: "Jin Kim"
date: "Saturday, April 25, 2015"
output:
  html_document:
    keep_md: true
    toc: true
    toc_depth: 2
---

```{r, message=FALSE, warning=FALSE}
source("dbook.R")
check.and.install.packages("dplyr")
check.and.install.packages("ggplot2")
check.and.install.packages("tidyr")
check.and.install.packages("ellipse")
check.and.install.packages("ridge")
check.and.install.packages("leaps")
check.and.install.packages("randomForest")
library(ggplot2)
library(plyr)
library(ellipse)
library(tidyr)
library(rpart)
library(MASS)
library(ridge)
library(randomForest)
library(leaps)
library(dplyr)
```

```{r}
summary(Boston)
#write.table(Boston, "ch3/boston.txt", sep="\t", row.names=F)
```

# Exploring the Data

```{r, echo=FALSE}
plot(Boston[,c(1:6, 14)])
plot(Boston[,c(7:13, 14)])

ctab = cor(Boston)
colorfun <- colorRamp(c("#CC0000","white","#3366CC"), space="Lab")
plotcorr(ctab, col=rgb(colorfun((ctab+1)/2), maxColorValue=255),
         mar = c(0.1, 0.1, 0.1, 0.1))

par(mfrow=c(1,2))
hist(Boston$medv, main="Density of Median Housing Prices (medv)")
plot(density(Boston$medv), main="Density of Median Housing Prices (medv)")
```

# Linear Model
```{r}
lm.b <- function(dt){
	res = lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, dt)
	res
}

lm.s <- function(dt){
	res = lm(medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat, dt)
	res
}

lm.r <- function(dt){
	res = linearRidge(medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + black + lstat, dt)
	res
}

```

```{r}
lm1 = lm.b(Boston)
summary(lm1)
rfb = rf.b(Boston)
summary(rfb)
rfb$importance
par(mfrow=c(1,2))
plot(Boston$medv, lm1$fitted.values, ylim=c(0,50), xlab="Actual Housing Prices", ylab="Predicted Housing Prices", main="Linear Regression")
lines(loess.smooth(Boston$medv, lm1$fitted.values))
plot(Boston$medv, rfb$predicted,  ylim=c(0,50), xlab="Actual Housing Prices", ylab="Predicted Housing Prices", main="RandomForest")
lines(loess.smooth(Boston$medv, rfb$predicted))
rmse(Boston$medv, lm1$fitted.values)
rmse(Boston$medv, rfb$predicted)
```

```{r}
```

## Variable Selection

Best Subset Selection

```{r}
vs.full = regsubsets(medv ~ ., Boston, nvmax = 13 )
vss.full = plot.vss(vs.full)
coef(vs.full, 11)
```



## Cross-validation

```{r, fig.width=10}
crt1 = cval.model(Boston, lm.b, 5)
crt2 = cval.model(Boston, lm.s, 5)
crt3 = cval.model(Boston, lm.r, 5)
par(mfrow=c(1,3))
plot(crt1$medv, crt1$res)
plot(crt2$medv, crt2$res)
plot(crt3$medv, crt3$res)
rmse(crt1$medv, crt1$res)
rmse(crt2$medv, crt2$res)
rmse(crt3$medv, crt3$res)
```

# Linear Model vs. RandomForest

## Model Definition
```{r}
rf.b <- function(dt){
	res = randomForest(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, dt)
	res
}
```

## Cross-validation
```{r}
crt1 = cval.model(Boston, lm.b, 3)
crt2 = cval.model(Boston, rf.b, 3)
par(mfrow=c(1,2))
plot(crt1$medv, crt1$res)
plot(crt2$medv, crt2$res)
rmse(crt1$medv, crt1$res)
rmse(crt2$medv, crt2$res)
```

```{r}
params = data.frame(k.fold = c(2, 3, 5, 10))
res = mdply(params, function(k.fold){
	cm1 = cval.model(Boston, lm.b, k.fold)
	cm2 = cval.model(Boston, rf.b, k.fold)
	data.frame(rmse.lm = rmse(cm1$medv, cm1$res), rmse.rf = rmse(cm2$medv, cm2$res))
})
res
```

## Varying the Amount of Training Data

```{r}
source("dbook.R")
rt = mdply(data.frame(rep = 1:10), function(rep){
		mdply(data.frame(ratio.train = 1:19/20), function(ratio.train){
		cm1 = eval.model(Boston, 14, lm.b, ratio.train)
		cm2 = eval.model(Boston, 14, rf.b, ratio.train)
		#browser()
		data.frame(rmse.elm = rmse(cm1$e$medv, cm1$e$res), rmse.erf = rmse(cm2$e$medv, cm2$e$res),
				   rmse.rlm = rmse(cm1$r$medv, cm1$r$res), rmse.rrf = rmse(cm2$r$medv, cm2$r$res))
	})
})
rts = group_by(rt, ratio.train) %>%
	summarise_each(funs(mean))
par(mfrow=c(1,2))
plot(rts$ratio.train, rts$rmse.elm, ylim=c(0,10), lty = 1, 
	 xlab = "% Train Set", ylab = "RMSE", main = "Linear Model") #Train vs. Test Set Errors
lines(rts$ratio.train, rts$rmse.rlm, lty=2)
plot(rts$ratio.train, rts$rmse.erf, ylim=c(0,10), lty = 1, 
	 xlab = "% Training Data", ylab = "RMSE", main = "RandomForest") #Train vs. Test Set Errors
lines(rts$ratio.train, rts$rmse.rrf, lty=2)

```


# Deprecated


```{r, eval=FALSE}
k.fold = 3
cvt = Boston
cvt$fold = sample(1:k.fold, nrow(cvt), replace=T)
crt = data.frame()
for(i in 1:k.fold){
	cvtr = cvt[cvt$fold != i,]
	cvte = cvt[cvt$fold == i,]
	cvte$res.b = predict(lm.b(cvtr, 14), cvte)
	cvte$res.e = predict(rf.b(cvtr, 14), cvte)
	crt = rbind(crt, cvte) 
}
par(mfrow=c(1,2))
plot(crt$medv, crt$res.b)
plot(crt$medv, crt$res.e)
cor(crt[,c("medv", "res.b", "res.e")])
rmse(crt$medv, crt$res.b)
rmse(crt$medv, crt$res.e)
```


```{r, eval=FALSE}
lm.r <- function(dt, idx.tgt){
	res = linearRidge(build.model(colnames(dt)[-idx.tgt], colnames(dt)[idx.tgt]), dt)
	res
}
lm.r(Boston, 14)
rt.b <- function(dt, idx.tgt){
	res = rpart(build.model(colnames(dt)[-idx.tgt], colnames(dt)[idx.tgt]), dt)
	res
}
rt.b(Boston, 14)
```

